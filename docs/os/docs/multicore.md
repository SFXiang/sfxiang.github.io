# 多核与多处理器

## 一、重点归纳总结

本章深入探讨了在多核与多处理器环境下，操作系统需要面对的硬件特性及其对软件性能、正确性的深刻影响。核心在于理解并利用**缓存一致性**、**内存一致性模型**和**非一致内存访问（NUMA）​**​ 这三大硬件特性，以构建高性能、高可扩展性的系统软件。

### 12.1 缓存一致性

尽管缓存一致性由硬件自动维护，对软件透明，但理解其原理对提升性能至关重要。

- ​**核心问题**​：多核私有缓存导致同一内存地址的数据可能存在多个副本，硬件必须确保这些副本的一致性，维护共享内存的正确抽象。
    
- ​**目录式协议（以MSI为例）​**​：通过一个全局目录跟踪每个缓存行的状态（**M**odified, ​**S**hared, ​**I**nvalid）和持有者。
    
    - ​**M（独占修改）​**​：仅当前核心有副本，且已修改（脏数据）。
        
    - ​**S（共享）​**​：多个核心有只读副本。
        
        - ​**I（无效）​**​：副本已过期。
            
        
    - 读写操作会触发状态迁移和核间通信（如使无效化消息）。
        
    
- ​**对系统软件的启示**​：
    
    1. ​**缓存行竞争**​：多个核心高频修改同一缓存行（如自旋锁的锁变量）会导致严重的性能下降。缓存行在核间频繁传递，使得访问串行化，无法发挥多核优势。
        
    2. ​**伪共享**​：不同核心访问同一缓存行中的_不同_变量，会引发不必要的缓存一致性流量。需通过内存对齐（如 `alignas(64)`）将不相关的数据隔离到不同的缓存行。
        
    3. ​**局部性原理**​：差的局部性不仅导致本核缓存命中率低，还会污染共享的末级缓存，影响整个系统。
        
    

### 12.2 内存一致性与硬件内存屏障

内存模型定义了多核环境下内存操作可见性的顺序规则，对软件_不透明_，是正确编程的基础。

- ​**访存乱序的根源**​：现代处理器使用**写缓冲区**和**读缓冲区**来隐藏访存延迟。一个核心的写操作在放入写缓冲区后即可视为“完成”，但数据可能尚未全局可见。这会导致其他核心观测到的操作顺序与程序顺序不一致。
    
- ​**内存模型由强到弱**​：
    
    - ​**顺序一致性**​：最直观，所有核看到的操作顺序一致，且每个核自身的操作顺序得到保持。
        
    - ​**TSO**​：允许“写读”乱序。x86/x86-64架构采用此模型。
        
        - ​**弱序**​：允许所有无依赖的访存操作乱序。ARM架构采用此模型。
            
        
    
- ​**内存屏障**​：在弱内存模型中，必须使用内存屏障指令来强制保证特定操作的顺序。
    
    - 例如，在基于共享内存的消息传递中，生产者写完数据后需要屏障，再写标志位；消费者读到标志位后需要屏障，再读数据。这确保了数据的可见性。
        
    
- ​**硬件实现差异**​：
    
    - ​**x86**​：写缓冲区是FIFO，保证了写写顺序。主要通过 `mfence`指令解决写读乱序。
        
    - ​**ARM**​：缓冲区无顺序保证，需频繁使用 `dmb`等屏障指令。也可通过构造数据依赖来保序。
        
    

### 12.3 非一致内存访问

NUMA架构通过分布内存控制器来扩展内存带宽，但引入了访问延迟的不均匀性。

- ​**核心概念**​：处理器访问本地内存（在同一NUMA节点内）速度远快于访问远程内存（在其他NUMA节点）。
    
- ​**性能影响**​：
    
    - ​**高延迟**​：远程访问延迟可能是本地访问的2-3倍或更高。
        
    - ​**低带宽**​：互联总线带宽成为瓶颈，远程访问带宽可能比本地低40%。
        
    
- ​**对操作系统的要求**​：OS需提供NUMA感知的抽象，如将内存分配在任务运行的本地节点，调度时避免跨节点迁移，以最大化局部性。
    

### 12.4 操作系统性能可扩展性

本节探讨如何针对上述硬件特性设计可扩展的系统软件，以互斥锁为例。

- ​**可扩展性问题**​：理想情况下，核数增加N倍，性能提升N倍。但受阿姆达尔定律和硬件开销限制。
    
- ​**缓存行竞争与MCS锁**​：
    
    - ​**传统自旋锁问题**​：所有核心争抢同一个锁变量所在的缓存行，高争用时缓存一致性开销巨大，性能急剧下降。
        
    - ​**MCS锁解决方案**​：将等待队列分布式化，每个竞争者在一个独立的、按缓存行对齐的节点上自旋。
        
        - ​**加锁**​：通过原子操作将自身节点加入队列尾部。
            
        - ​**解锁**​：锁持有者直接通知队列中的下一个竞争者，而不是广播。
            
        - ​**优势**​：将全局缓存行竞争转化为局部的、点对点的通信，缓存失效次数与竞争者数量无关，可扩展性极佳。
            
        
    
- ​**NUMA环境与Cohort锁**​：
    
    - ​**MCS锁在NUMA下的新问题**​：若竞争者分布在不同NUMA节点，锁传递会导致锁元数据和临界区数据在节点间迁移，产生高延迟的远程访问。
        
    - ​**Cohort锁解决方案**​：采用两层结构（全局锁 + 每NUMA节点一个本地锁）。
        
        - 当某个节点获得全局锁后，锁会在该节点内的竞争者间连续传递一段时间，再释放全局锁给下一个节点。
            
        - ​**优势**​：将一段时间内的锁竞争限制在节点内部，避免了频繁的跨节点通信，牺牲一定公平性换取更好的NUMA可扩展性。
            
        
    

### 12.5 案例分析：Linux内核中的NUMA感知设计

- ​**NUMA感知的内存管理**​：提供 `set_mempolicy`/`mbind`等系统调用，允许应用指定内存分配策略（如绑定到特定节点、优先本地、交错分配）。对于未显式指定的应用，内核尽力在本地节点分配内存。
    
- ​**NUMA感知的调度**​：引入**调度域**层次结构（如核内、CPU包内、NUMA节点内、全系统）。负载均衡策略与域层级相关，越低层（迁移开销小）的域进行负载均衡越频繁，越高层（如跨NUMA节点，迁移开销大）的域越谨慎，从而在负载均衡和保持内存局部性之间取得平衡。
    

---

## 二、思考题详细解答

### 1. 在目录式缓存一致性协议中，为了避免单一的共享目录成为瓶颈，目录该如何设计？

​**答案**​：

为了避免中央目录成为性能和可扩展性的瓶颈，可以采用以下分布式设计思路：

1. ​**分布式目录**​：将全局目录与内存一同分布式地部署在各个处理节点上。每个节点负责跟踪其本地内存块的缓存一致性状态。当一个处理器访问某个内存地址时，请求会被发送到该地址的**宿主节点**的目录。这样，目录管理和通信的压力就被分摊到了各个节点上。
    
2. ​**稀疏目录/目录缓存**​：不是所有内存块都同时被缓存。可以只为当前正被缓存的内存块分配目录项，而不是为所有物理内存块维护全目录。这通常通过一个容量较小的**目录缓存**来实现，其可以是片上最后一级缓存内容的超集（包容性目录）。
    
3. ​**有限指针目录**​：目录项中不记录所有共享者的完整位图，而是使用固定数量的指针（如4个）来记录一部分共享者。当共享者数量超过指针数时，可以采用替换策略（如LRU）或降级为广播使无效消息。这是在存储开销和精确性之间的权衡。
    
4. ​**链式目录**​：目录项只保存一个指针，指向一个由共享者信息构成的链表。这种方式存储效率最高，但遍历链表的延迟较大。
    

### 2. 在现代处理器的缓存一致性协议的设计中，缓存行除了我们介绍的MSI状态，通常还添加了其他的状态如独占未修改状态（Exclusive）。添加新的状态会显著提高处理器设计的复杂性，请分析为何现代处理器会采用这种设计？

​**答案**​：

添加E状态形成**MESI**协议，是为了优化一个极其常见且影响性能的场景：​**一个核心读入数据后随后修改它，且在此期间没有其他核心缓存该数据**。

- ​**在MSI协议下**​：即使只有该核心操作此数据，其写入操作也需要发送“读使无效”消息，以确认没有其他副本。这是一次不必要的总线事务和延迟。
    
- ​**在MESI协议下**​：
    
    - 当核心读入数据，且通过总线嗅探确认无其他缓存副本时，可将状态置为**E**。
        
    - 当该核心要写入这个处于E状态的数据时，它**无需任何总线通信**，可直接在本地将状态改为M并写入。
        
    
- ​**权衡**​：虽然E状态增加了状态转换的逻辑复杂性，但它**精准地优化了无竞争写的性能**，消除了那次昂贵的总线事务，降低了延迟，减轻了总线负担。对于提升常见程序的性能收益巨大，因此现代高性能处理器普遍认为值得为此付出设计复杂性的代价。
    

### 3. 在MCS锁的lock流程中，是否需要在没有人拿锁时（tail == NULL，第16行），添加me->flag = GRANTED以表示当前线程被允许进入临界区？

​**答案**​：

​**不需要，且不能添加**。

- ​**原因分析**​：当 `tail == NULL`时，意味着当前线程是第一个也是唯一的竞争者。它通过 `atomic_XCHG`操作将 `lock->tail`指向自己，并发现之前的 `tail`是 `NULL`，这表示它成功获得了锁。
    
- ​**关键点**​：此时，该线程**必须能够立即进入临界区**，而不应等待任何条件（包括检查自己的 `flag`）。如果在此处设置 `me->flag = GRANTED`，从逻辑上看似乎合理，但实际上是冗余的。更重要的是，​**整个MCS锁的解锁逻辑依赖于 `flag`来传递锁**。如果第一个线程在加锁时自己设置 `flag`，会与解锁流程中“由前驱节点设置后继节点的 `flag`”这一核心机制产生混淆。
    
- ​**正确逻辑**​：第一个线程发现 `tail == NULL`后，应直接跳过等待循环，进入临界区。它的 `flag`状态可以是任意值（通常是初始的 `WAITING`），因为它在这次加锁过程中根本不会检查自己的 `flag`。锁的传递机制是从第二个竞争者开始才生效的。
    

### 4. 如果分别采用TSO一致性模型与弱序一致性模型，在MCS锁的lock流程中，tail->next = me前（第16、17行）是否需要添加内存屏障？为什么？

​**答案**​：

- ​**在TSO模型（如x86）下**​：​**通常不需要显式屏障**。TSO保证写写顺序。操作 `tail = atomic_XCHG(&lock->tail, me)`（产生一个写）和随后的 `tail->next = me`（另一个写）之间的顺序是得到保证的。确保在将新节点链接到队列尾部之前，`atomic_XCHG`的结果（即前驱节点）对其他核心是可见的。
    
- ​**在弱序模型（如ARM）下**​：​**必须添加写屏障**。弱序模型不保证无依赖的写写顺序。编译器或处理器可能会将 `tail->next = me`重排到 `atomic_XCHG`之前执行。这将是灾难性的：如果当前线程先设置了前驱节点的 `next`指针，但前驱节点还未通过 `atomic_XCHG`感知到当前线程的存在，那么前驱节点在解锁时可能认为自己是队列末尾而直接释放锁，导致当前线程永远无法被唤醒。因此，必须在 `atomic_XCHG`和 `tail->next = me`之间插入一个写屏障（如ARM的 `dmb ishst`），确保链接操作不会被重排到交换操作之前。
    

### 5. 对于代码段8.8介绍的排号锁，是否也会出现可扩展性问题？如何简要修改该代码来使其可扩展性变好？

​**答案**​：

- ​**是否会出现可扩展性问题**​：​**会**。排号锁（Ticket Lock）虽然保证了公平性，但其性能可扩展性同样很差。它的核心问题与简单自旋锁类似：所有释放锁的核心都会去写同一个全局变量 `service_num`以更新叫号器。这导致高争用时，`service_num`所在的缓存行在所有等待核心间剧烈抖动，产生巨大的缓存一致性开销。
    
- ​**简要修改方案**​：可以借鉴MCS锁的思想，将其改造成**队列锁**。具体来说：
    
    1. 为每个竞争者创建一个本地变量（或节点）用于自旋。
        
    2. 使用一个原子操作将当前竞争者排入一个全局的等待队列（一个链表），并获取其前驱节点。
        
    3. 竞争者在其本地变量上自旋，等待前驱节点释放锁后通知它。
        
    4. 锁的释放者只需通知队列中的下一个节点，而不是广播给所有核心。
        
        这种改造将全局缓存行竞争转化为局部的点对点通信，从而获得类似MCS锁的良好可扩展性。Linux内核中的 `qspinlock`就是结合了排号锁的公平性和队列锁可扩展性的一个成功实践。
        
    

### 6. 在偏向写者的读写锁中，读者需要累加一个全局计数器来避免写者进入临界区。因此这个全局计数器所在缓存行会被不同核上的读者与写者竞争，其暴露的开销会在读者的关键路径上。请描述一下如何修改设计可以避免这个问题？

​**答案**​：

读者计数器是读写锁的一个常见瓶颈。改进设计的核心思路是**减少对共享计数器的争用**。

1. ​**分布式读者计数器**​：不再使用单一的全局计数器。可以为每个核心或每个NUMA节点维护一个本地读者计数器。读者进入时，只增加本地的计数器。写者需要进入临界区时，则必须遍历所有本地计数器，检查其总和是否为零。这被称为 ​**​“懒惰的”或“分布式”读写锁**。
    
    - ​**优点**​：读操作变得非常快，几乎没有争用。
        
    - ​**缺点**​：写操作开销变大，需要检查所有本地计数器，但这种情况在“读多写少”的场景下是可接受的。
        
    
2. ​**使用RCU**​：对于读操作极其频繁、写操作非常少的场景，可以考虑使用RCU机制。RCU允许读者在没有任何原子操作或内存屏障的情况下进行，性能极高。写者则需要承担更复杂的同步开销来安全地更新数据。
    

### 7. 既然对单一缓存行竞争会产生严重的可扩展性问题，那我们是不是应该将所有的共享数据都放在不同的缓存行来避免这个问题？

​**答案**​：

​**不是。这是一个需要权衡的优化策略，不能盲目使用。​**​

- ​**优点**​：对于**高频修改的独立变量**​（如每CPU计数器、锁变量），将其隔离到独立的缓存行可以彻底消除伪共享，带来显著的性能提升。
    
- ​**缺点**​：
    
    1. ​**内存空间浪费**​：缓存行通常为64字节。如果为一个4字节的整数进行填充，有效数据只占6.25%，大幅增加了内存占用。这可能导致缓存中能存放的有效数据量减少，反而可能引发更多的缓存缺失。
        
    2. ​**破坏局部性**​：缓存行设计的初衷是利用“空间局部性”。如果将逻辑上紧密相关、经常被一起访问的数据（如结构体字段、数组元素）强制分散到不同的缓存行，就破坏了这种局部性。处理器需要加载更多的缓存行才能完成相同的工作，效率可能下降。
        
    
- ​**正确做法**​：应**基于数据的访问模式进行设计**。
    
    - 将**访问模式相似**的数据成员组织在一起（例如，会被同一线程频繁访问的数据放在一个结构里）。
        
    - 只为那些已被证实存在激烈缓存行竞争的、被不同线程频繁写入的“热点”变量进行缓存行隔离。这通常需要通过性能剖析工具来定位。
        
    

### 8. 我们在12.4.2节介绍的cohort锁是否能直接用在操作系统内核中？你认为有哪些方面的问题需要考虑？

​**答案**​：

不能直接使用，需要针对内核环境进行大量适配和权衡。

1. ​**内存分配**​：Cohort锁需要为每个NUMA节点预先分配一个本地锁结构。在内核中，动态内存分配是受限的，尤其是在中断上下文等场景下不能睡眠。需要设计安全的内存分配机制。
    
2. ​**锁的粒度与数量**​：内核中有成千上万种锁保护不同的资源。为每一种锁都实现一套Cohort机制会带来巨大的内存开销和复杂性。需要判断哪些锁是高频争用的、值得用Cohort来优化。
    
3. ​**中断/异常处理**​：内核代码可能被中断打断。如果中断处理程序也试图获取同一个Cohort锁，可能导致死锁或其他复杂情况。需要仔细处理中断上下文中的锁获取。
    
4. ​**负载适应性**​：Cohort锁在本地有竞争者时性能好。但如果锁的争用本来就不高，或者竞争者总是分布在不同的NUMA节点上，Cohort锁的额外层级开销可能比简单的自旋锁还大。内核可能需要一种能根据争用情况动态切换锁策略的机制（类似 `qspinlock`）。
    
5. ​**公平性与饥饿**​：Cohort锁优先服务本地节点的竞争者，可能导致远程节点的线程饥饿。内核需要引入防饥饿机制，例如限制锁在单个节点内连续传递的最大次数。
    

### 9. 开放问题：在本章介绍了多核硬件的各种局限性之后，请举具体例子，什么样的应用适合使用多核多处理器进行计算？什么样的应用更适合使用单核进行计算？

​**答案**​：

- ​**适合多核/多处理器的应用**​：
    
    - ​**特征**​：具有**高并行度、可分解**的任务，且任务间的**通信和同步开销相对较低**。
        
    - ​**例子**​：
        
        1. ​**Web服务器**​：每个用户请求可以独立处理，是典型的“易并行”任务。
            
        2. ​**科学计算（如气候模拟、蛋白质折叠）​**​：可以将大的计算域（如三维网格）分割成多个小块，分给不同核心计算，只需在边界进行数据同步。
            
        3. ​**视频编码/渲染**​：可以将视频的不同帧或图像的不同区域分配给不同核心并行处理。
            
        4. ​**数据分析（如MapReduce）​**​：Map阶段可以高度并行，Reduce阶段再进行汇总。
            
        
    
- ​**适合单核的应用**​：
    
    - ​**特征**​：​**任务本质串行**、**并行化后同步开销极大**、或**对单任务响应延迟有极致要求**。
        
    - ​**例子**​：
        
        1. ​**复杂的决策逻辑**​：例如，围棋AI的蒙特卡洛树搜索，虽然可以并行模拟，但树结构的更新需要频繁加锁，可能使并行效率极低。
            
        2. ​**强依赖共享状态的流水线**​：后一个任务严重依赖于前一个任务的全部结果，难以重叠执行。
            
        3. ​**高频率交易系统**​：虽然可以多线程处理不同交易，但为了追求极致的单次交易延迟，避免任何核间通信开销（如缓存同步、锁争用），可能会将关键任务绑定到单个核心，并独占缓存。
            
        4. ​**内存带宽受限型应用**​：如果应用本身已经是内存带宽的瓶颈，那么增加更多核心只会争抢有限的内存带宽，而无法提升性能。
            
        
    

​**结论**​：选择多核还是优化单核性能，取决于应用的**并行潜力**与**并行开销**之间的权衡。阿姆达尔定律是指导这一决策的基本法则。